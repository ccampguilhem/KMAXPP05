{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bd7a19-8e8e-45c0-8126-bb28c26c4a01",
   "metadata": {},
   "source": [
    "# Régression probabiliste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96ba99-1e15-4bd2-9f80-f752d20a862c",
   "metadata": {},
   "source": [
    "Ce notebook est inspiré par les notes de cours de Fabrice Gamboa et des planches de Carl Edward Rasmussen et Zoubin Ghahramani : \n",
    "* https://mlg.eng.cam.ac.uk/teaching/4f13/1314/lect0102.pdf\n",
    "* https://mlg.eng.cam.ac.uk/teaching/4f13/1314/lect0304.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d0290-5d8f-43de-9ebd-46b047cd6ece",
   "metadata": {},
   "source": [
    "### 1. Rappel de la régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92edf35-683b-4dcd-81f8-b67afa22681a",
   "metadata": {},
   "source": [
    "On suppose la structure de modèle $\\mathcal{M}$ suivante : \n",
    "\\begin{align*}\n",
    "    y_i & = \\beta_0 + \\beta_1 x_i^1 + \\beta_2 x_i^2 + \\ldots + \\beta_d x_i^d + \\epsilon_i; & \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\\\\n",
    "                          & = \\boldsymbol{\\phi}(x_i)^T \\boldsymbol{\\beta} + \\epsilon_i & \n",
    "\\end{align*}\n",
    "où \n",
    "$$\\boldsymbol{\\phi}(x_i) = \\begin{bmatrix} 1 \\\\ x_i \\\\ x_i^2 \\\\ \\vdots \\\\ x_i^d \\end{bmatrix} \\:\\:\\:\\:\\:\\:\\:\\: et \\:\\:\\:\\:\\:\\:\\:\\: \\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_d \\end{bmatrix}$$\n",
    "\n",
    "Notons $\\boldsymbol{x} = [x_1 \\ldots x_N]^T$ les variables explicatives et $\\boldsymbol{y} = [y_1 \\ldots y_N]^T$ les variables à prédire de la base d'apprentissage qui contient $N$ échantillons. \n",
    "\n",
    "On définit la matrice $\\boldsymbol{\\phi}$ comme suit :  \n",
    "$$\\boldsymbol{\\phi} = \\begin{bmatrix} \\boldsymbol{\\phi}(x_1)^T \\\\ \\vdots \\\\ \\boldsymbol{\\phi}(x_N)^T\\end{bmatrix}$$\n",
    "\n",
    "Par l'hypothèse de normalité, on rappelle que la vraisemblance $p(\\boldsymbol{y} \\vert \\boldsymbol{x}, \\boldsymbol{\\beta}, \\mathcal{M})$ est définie comme suit :\n",
    "\\begin{align*}\n",
    "    p(\\boldsymbol{y} \\vert \\boldsymbol{x}, \\boldsymbol{\\beta}, \\mathcal{M}) = \\mathcal{N}(\\boldsymbol{y}; \\boldsymbol{\\phi}\\boldsymbol{\\beta}, \\sigma^2\\boldsymbol{I})\n",
    "\\end{align*}\n",
    "\n",
    "On obtient l'estimation des paramètres du modèle par la méthode du maximum vraisemblance :\n",
    "$$ \\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} p(\\boldsymbol{y} \\vert \\boldsymbol{x}, \\boldsymbol{\\beta}, \\mathcal{M})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371363fc-001e-4279-8976-66c7ec447826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; np.random.seed(3)\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07cf7c-3975-4119-90ba-0859f6154bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x_train = 3 * np.random.rand(n) - 1.5\n",
    "y_train = 0.5 * x_train - 0.08 * x_train**2 - 0.05 * x_train**3 + 0.5 * np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f17bb-e8e9-4c40-b8dd-67e73910a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A Python class that defines a linear model for regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_features(X, degree):\n",
    "        features = [X**k for k in range(degree+1)]\n",
    "        features = np.stack(features, axis=1)\n",
    "        return features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        XTX = np.matmul(X.T, X)\n",
    "        assert np.linalg.det(XTX) != 0\n",
    "        self.beta_hat = np.linalg.inv(XTX).dot(X.T).dot(y)\n",
    "\n",
    "    def pred(self, X):\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        y_hat = X.dot(self.beta_hat)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b3ae7-e47a-4a78-a403-4fee1615d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize linear models w/ various polynomial degrees\n",
    "models = []\n",
    "degrees = [0, 1, 2, 6]\n",
    "\n",
    "for degree in degrees:\n",
    "    model = LinearModel(degree=degree)\n",
    "    model.fit(x_train, y_train)\n",
    "    models.append(model)\n",
    "\n",
    "# Make predictions over the training range\n",
    "x_test = np.linspace(x_train.min() - 0.1, x_train.max() + 0.1, 100)\n",
    "y_preds = []\n",
    "for model in models:\n",
    "    y_preds.append(model.pred(x_test))\n",
    "\n",
    "# Plot the predictions\n",
    "colors = sns.color_palette(\"husl\", len(degrees))\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 2))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax[i].scatter(x_train, y_train)\n",
    "    ax[i].plot(x_test, y_preds[i], color=colors[i])\n",
    "    ax[i].set_title('Degree: {}'.format(degree))\n",
    "    ax[i].set_xlabel('x')\n",
    "    if i == 0:\n",
    "        ax[i].set_ylabel(r'$\\hat{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b72d5-33bc-4767-97eb-d3c3a6eda8cc",
   "metadata": {},
   "source": [
    "### 2. Régression probabiliste : une approche Bayésienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926ced4-805c-4a9c-8632-6f27f85ce99f",
   "metadata": {},
   "source": [
    "On change de paradigme : on suppose qu'il n'existe plus un seul jeu de paramètres à estimer, mais que plusieurs paramètres sont vraisemblables.\n",
    "\n",
    "Supposons la structure de modèle $\\mathcal{M}$ suivante : \n",
    "\\begin{align*}\n",
    "    y_i & = \\beta_0 + \\beta_1 x_i^1 + \\beta_2 x_i^2 + \\ldots + \\beta_6 x_i^6 + \\epsilon_i; & \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\\\\n",
    "                          & = \\boldsymbol{\\phi}(x_i)^T \\boldsymbol{\\beta} + \\epsilon_i & \n",
    "\\end{align*}\n",
    "Formulons un *a priori* sur la distribution des paramètres : \n",
    "$$p(\\boldsymbol{\\beta} \\vert \\mathcal{M}) = \\mathcal{N}(\\boldsymbol{0}, \\sigma_\\beta^2 \\boldsymbol{I})$$\n",
    "Sans prendre en compte les observations, on peut générer des prédictions à partir des paramètres du modèle qui sont *a priori* vraisemblables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df920fd-2ac3-429e-8ada-c5948395a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticLinearModel:\n",
    "    \"\"\"\n",
    "    A Python class that defines a linear model for regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree, prior_mean=0, prior_std=1):\n",
    "        self.degree = degree\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_std = prior_std\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_features(X, degree):\n",
    "        features = [X**k for k in range(degree+1)]\n",
    "        features = np.stack(features, axis=1)\n",
    "        return features\n",
    "\n",
    "    def sample_prior(self):\n",
    "        beta_prior = np.random.normal(\n",
    "            self.prior_mean * np.ones(self.degree + 1), \n",
    "            self.prior_std**2 * np.ones(self.degree + 1)\n",
    "        )\n",
    "        return beta_prior\n",
    "\n",
    "    def pred(self, X, fitted=True):\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        if fitted:\n",
    "            beta_hat = self.sample_posterior()\n",
    "        else:\n",
    "            beta_hat = self.sample_prior()\n",
    "        y_hat = X.dot(beta_hat)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24654e8-b7e1-4d27-a22a-b471b82c4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_model = ProbabilisticLinearModel(degree=8, prior_mean=0, prior_std=1)\n",
    "n_samples = 4\n",
    "# Make predictions over the training range\n",
    "x_test = np.linspace(x_train.min() - 0.1, x_train.max() + 0.1, 100)\n",
    "y_preds = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    y_preds.append(prob_model.pred(x_test, fitted=False))\n",
    "\n",
    "# Plot the predictions\n",
    "colors = sns.color_palette(\"husl\", n_samples)\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 2))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    ax[i].plot(x_test, y_preds[i], color=colors[i])\n",
    "    ax[i].set_title('Degree: {}'.format(prob_model.degree))\n",
    "    ax[i].set_xlim(x_test[0], x_test[-1])\n",
    "    ax[i].set_ylim(-4, 4)\n",
    "    ax[i].set_xlabel('x')\n",
    "    if i == 0:\n",
    "        ax[i].set_ylabel(r'$\\hat{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c79ed26-45fa-44a3-beef-eb2f6d15c0d9",
   "metadata": {},
   "source": [
    "Etant donné les observations, on va maintenant \"ajuster\" les paramètres du modèle.\n",
    "\n",
    "D'après la loi de Bayes, on a :\n",
    "\\begin{align*}\n",
    "    p( \\boldsymbol{\\beta} \\vert  \\boldsymbol{y}, \\boldsymbol{x},\\mathcal{M}) = \\frac{p(\\boldsymbol{y} \\vert \\boldsymbol{x}, \\boldsymbol{\\beta}, \\mathcal{M})p(\\boldsymbol{\\beta}, \\mathcal{M})}{p(\\boldsymbol{y} \\vert \\boldsymbol{x}, \\mathcal{M})}\n",
    "\\end{align*}\n",
    "où $p(\\boldsymbol{y} \\vert \\boldsymbol{x}, \\mathcal{M}) = \\int p(\\boldsymbol{y}, \\boldsymbol{\\beta} \\vert \\boldsymbol{x}, \\mathcal{M})d\\boldsymbol{\\beta} = \\int  p(\\boldsymbol{y} \\vert \\boldsymbol{x}, \\boldsymbol{\\beta}, \\mathcal{M})p(\\boldsymbol{\\beta}, \\mathcal{M}) d\\boldsymbol{\\beta}$\n",
    "\n",
    "De plus, comme la distribution de vraisemblance et la distribution *a priori* sont gaussiennes, la distribution *a posteriori* est également gaussienne : \n",
    "\\begin{align*}\n",
    "    p( \\boldsymbol{\\beta} \\vert  \\boldsymbol{y}, \\boldsymbol{x}, \\mathcal{M}) & = \\mathcal{N}(\\boldsymbol{\\beta} ; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma)} \\\\\n",
    "    \\mbox{avec } \\boldsymbol{\\mu} = (\\boldsymbol{\\phi}^T\\boldsymbol{\\phi} + \\frac{\\sigma^2}{\\sigma_{\\beta}^2} \\boldsymbol{I})^{-1}\\boldsymbol{\\phi}^T\\boldsymbol{y} \\:\\:\\:\\:\\: & \\mbox{ et }  \\:\\:\\:\\:\\: \\boldsymbol{\\Sigma} = (\\sigma^{-2}\\boldsymbol{\\phi}^T\\boldsymbol{\\phi} + \\sigma_\\beta^{-2}\\boldsymbol{I})^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Ici, on n'obtient pas d'estimation ponctuelle, *e.g.* du maximum de vraisemblance. A la place, le loi de Bayes nous donne une distribution *a posteriori* sur les paramètres du modèle. Elle définit les paramètres qui sont vraisemblables compte tenu des données. Ci-dessous, on échantillonne plusieurs paramètres $\\boldsymbol{\\beta}$ selon la distribution *a posteriori*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e73d59-f319-4593-b0df-2d0bac31c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticLinearModel:\n",
    "    \"\"\"\n",
    "    A Python class that defines a probabilistic linear model for regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree, prior_mean=0, prior_std=1):\n",
    "        self.degree = degree\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_std = prior_std\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_features(X, degree):\n",
    "        features = [X**k for k in range(degree+1)]\n",
    "        features = np.stack(features, axis=1)\n",
    "        return features\n",
    "\n",
    "    def sample_prior(self):\n",
    "        beta_prior = np.random.normal(\n",
    "            self.prior_mean * np.ones(self.degree + 1), \n",
    "            self.prior_std**2 * np.ones(self.degree + 1)\n",
    "        )\n",
    "        return beta_prior\n",
    "\n",
    "    def pred(self, X, fitted=True):\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        if fitted:\n",
    "            beta_hat = self.sample_posterior()\n",
    "        else:\n",
    "            beta_hat = self.sample_prior()\n",
    "        y_hat = X.dot(beta_hat)\n",
    "        return y_hat\n",
    "\n",
    "    def estimate_noise(self, X, y):\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        XTX = np.matmul(X.T, X)\n",
    "        assert np.linalg.det(XTX) != 0\n",
    "        beta_hat = np.linalg.inv(XTX).dot(X.T).dot(y)\n",
    "        y_hat = X.dot(beta_hat)\n",
    "        noise = np.sum((y - y_hat)**2) / (X.shape[0] - X.shape[1])\n",
    "        return noise\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.noise = self.estimate_noise(X, y)\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        XTX = np.matmul(X.T, X)\n",
    "        assert np.linalg.det(XTX) != 0\n",
    "        self.mean_posterior = np.linalg.inv(\n",
    "            XTX + self.noise / (self.prior_std**2) * np.eye(X.shape[1])\n",
    "        ).dot(X.T).dot(y)\n",
    "        self.cov_posterior = np.linalg.inv(\n",
    "            XTX / self.noise + (1 / (self.prior_std**2)) * np.eye(X.shape[1])\n",
    "        )\n",
    "\n",
    "    def sample_posterior(self):\n",
    "        return np.random.multivariate_normal(self.mean_posterior, self.cov_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791cb8c-db65-495c-8461-22ce313b6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_model = ProbabilisticLinearModel(degree=5, prior_mean=0, prior_std=1)\n",
    "prob_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a3e4f-38a5-4f36-96f0-70c8595763f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions over the training range\n",
    "y_preds = []\n",
    "n_samples = 4\n",
    "for i in range(n_samples):\n",
    "    y_preds.append(prob_model.pred(x_test, fitted=True))\n",
    "\n",
    "# Plot the predictions\n",
    "colors = sns.color_palette(\"husl\", n_samples)\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 2))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    ax[i].scatter(x_train, y_train)\n",
    "    ax[i].plot(x_test, y_preds[i], color=colors[i])\n",
    "    ax[i].set_title('Degree: {}'.format(prob_model.degree))\n",
    "    ax[i].set_xlabel('x')\n",
    "    if i == 0:\n",
    "        ax[i].set_ylabel(r'$\\hat{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e05f34-c589-4cfa-bc4d-5b8947a9596f",
   "metadata": {},
   "source": [
    "On peut calculer la moyenne et l'écart-type empirique de plusieurs prédictions réalisées à partir de différents paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07195c39-8895-4b6f-86d8-48db32b24bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions over the training range\n",
    "x_test = np.linspace(x_train.min() - 0.3, x_train.max() + 0.3, 100)\n",
    "y_preds = []\n",
    "n_samples = 100\n",
    "for i in range(n_samples):\n",
    "    y_preds.append(prob_model.pred(x_test, fitted=True))\n",
    "y_preds = np.stack(y_preds)\n",
    "mean_pred = np.mean(y_preds, axis=0)\n",
    "std_pred = np.std(y_preds, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(x_train, y_train)\n",
    "mean = ax.plot(x_test, mean_pred, color='black', label='Moyenne')\n",
    "ax.fill_between(x_test, mean_pred + std_pred, mean_pred - std_pred, alpha=0.2, color='orange')\n",
    "for i in range(n_samples):\n",
    "    ax.plot(x_test, y_preds[i], alpha=0.04, color='black')\n",
    "patch = mpatches.Patch(color='orange', alpha=0.2, label=r'Moyenne $\\pm$ écart-type')\n",
    "plt.legend(handles=[mean[0], patch])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(r'$\\hat{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e7b5b-4476-4a61-a02b-667e454c9b6f",
   "metadata": {},
   "source": [
    "Mais dans le cas où l'on modélise la distribution *a priori* et la vraisemblance par des Gaussiennes, on peut calculer analytiquement la distribution conditionnelle de la variable à prédire sachant la variable explicative, les données d'apprentissage, et la structure du modèle :\n",
    "$$p(y^\\ast \\vert x^\\ast, \\boldsymbol{x}, \\boldsymbol{y}, \\mathcal{M}) = \\mathcal{N}\\big(y^\\ast; \\boldsymbol{\\phi}(x^\\ast)^T\\boldsymbol{\\mu}, \\boldsymbol{\\phi}(x^\\ast)^T \\boldsymbol{\\Sigma} \\boldsymbol{\\phi}(x^\\ast) + \\sigma^2\\big) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff730f-6869-44b3-ade0-0da117d7e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticLinearModel:\n",
    "    \"\"\"\n",
    "    A Python class that defines a probabilistic linear model for regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree, prior_mean=0, prior_std=1):\n",
    "        self.degree = degree\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_std = prior_std\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_features(X, degree):\n",
    "        features = [X**k for k in range(degree+1)]\n",
    "        features = np.stack(features, axis=1)\n",
    "        return features\n",
    "\n",
    "    def sample_prior(self):\n",
    "        beta_prior = np.random.normal(\n",
    "            self.prior_mean * np.ones(self.degree + 1), \n",
    "            self.prior_std**2 * np.ones(self.degree + 1)\n",
    "        )\n",
    "        return beta_prior\n",
    "\n",
    "    def pred(self, X, fitted=True):\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        if fitted:\n",
    "            beta_hat = self.sample_posterior()\n",
    "        else:\n",
    "            beta_hat = self.sample_prior()\n",
    "        y_hat = X.dot(beta_hat)\n",
    "        return y_hat\n",
    "\n",
    "    def estimate_noise(self, X, y):\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        XTX = np.matmul(X.T, X)\n",
    "        assert np.linalg.det(XTX) != 0\n",
    "        beta_hat = np.linalg.inv(XTX).dot(X.T).dot(y)\n",
    "        y_hat = X.dot(beta_hat)\n",
    "        noise = np.sum((y - y_hat)**2) / (X.shape[0] - X.shape[1])\n",
    "        return noise\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.noise = self.estimate_noise(X, y)\n",
    "        X = self.polynomial_features(X, self.degree)\n",
    "        XTX = np.matmul(X.T, X)\n",
    "        assert np.linalg.det(XTX) != 0\n",
    "        self.mean_posterior = np.linalg.inv(\n",
    "            XTX + self.noise / (self.prior_std**2) * np.eye(X.shape[1])\n",
    "        ).dot(X.T).dot(y)\n",
    "        self.cov_posterior = np.linalg.inv(\n",
    "            XTX / self.noise + (1 / (self.prior_std**2)) * np.eye(X.shape[1])\n",
    "        )\n",
    "\n",
    "    def sample_posterior(self):\n",
    "        return np.random.multivariate_normal(self.mean_posterior, self.cov_posterior)\n",
    "\n",
    "    def pred_mean(self, x_star):\n",
    "        x = self.polynomial_features(x_star, self.degree)\n",
    "        return x.dot(self.mean_posterior)\n",
    "        \n",
    "    def pred_interval(self, x_star, p_int=0.95):\n",
    "        x = self.polynomial_features(x_star, self.degree)\n",
    "        mean = x.dot(self.mean_posterior)\n",
    "        cov = np.matmul(x, self.cov_posterior)[:, np.newaxis, :]\n",
    "        cov = np.matmul(cov, x[:, :, np.newaxis]).reshape(-1)\n",
    "        Q = norm.isf((1-p_int) / 2)\n",
    "        l = Q * np.sqrt(cov + self.noise)\n",
    "        low = mean - l\n",
    "        high = mean + l\n",
    "        return (low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b56ed3-e871-420f-9a55-ff1322a7e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_model = ProbabilisticLinearModel(degree=6, prior_mean=0, prior_std=1)\n",
    "prob_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions over the training range\n",
    "x_test = np.linspace(x_train.min() - 0.3, x_train.max() + 0.3, 100)\n",
    "pred_mean = prob_model.pred_mean(x_test)\n",
    "p_int = 0.95\n",
    "low, high = prob_model.pred_interval(x_test, p_int=p_int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(x_train, y_train)\n",
    "mean = ax.plot(x_test, pred_mean, color='black', label='Espérance')\n",
    "ax.fill_between(x_test, low, high, alpha=0.2, color='orange')\n",
    "patch = mpatches.Patch(color='orange', alpha=0.2, label=r'Intervalle de prédiction à {}%'.format(p_int*100))\n",
    "plt.legend(handles=[mean[0], patch])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(r'$\\hat{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6400a9-83be-42da-ae9c-736966a04d7f",
   "metadata": {},
   "source": [
    "### 3. Processus Gaussiens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfdc1f-7657-4761-90ca-b8cba1293b60",
   "metadata": {},
   "source": [
    "Ci-dessus, on a défini un *a priori* sur les paramètres du modèle linéaire, et donc indirectement sur le modèle en lui-même. Peut-on définir un *a priori* directement sur le modèle ? On va introduire la notion du processus Gaussien, qui est la généralisation d'une distribution Gaussienne multivariée. \n",
    "\n",
    "Tout d'abord, un processus aléatoire est une collection de variables aléatoires $Z(x)$, indexées par $x \\in \\mathcal{X}$. Dans la suite, $\\mathcal{X} = \\mathbb{R}$. \n",
    "\n",
    "Un processus Gaussien $Z$ est un processus aléatoire tel qu'une collection finie de ses variables a une distribution normale multivariée. En d'autres termes, pour une collection d'indices $\\{x_1, \\ldots, x_N\\}$, le vecteur $(Z(x_1), \\ldots, Z(x_N))$ est un vecteur Gaussien.\n",
    "\n",
    "$\\longrightarrow$ Une distribution Gaussienne multivariée de dimension $d$ est définie par une moyenne de dimension $d$ et une matrice de covariance de taile $d \\times d$. <br>\n",
    "$\\longrightarrow$ Un processus Gaussien $Z$ est défini par une fonction moyenne $m : \\mathcal{X} \\longrightarrow \\mathbb{R}$ et une fonction de covariance $k : \\mathcal{X} \\times \\mathcal{X} \\longrightarrow \\mathbb{R}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cebfb2-f92b-476e-9e4e-c449f4fea109",
   "metadata": {},
   "source": [
    "**Processus Gaussien pour la régression**\n",
    "\n",
    "On va modéliser la relation entre les variables explicatives et la variable à prédire par un processus Gaussien :\n",
    "$$y_i = Z(x_i) + \\epsilon_i \\:\\:\\:\\: ; \\:\\:\\:\\: \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5186b22-658d-4d41-b57a-399e707ce467",
   "metadata": {},
   "source": [
    "**Exemple de processus Gaussien** \n",
    "\n",
    "On fait le choix d'une structure de modèle $\\mathcal{M}$ caractérisée par la fonction moyenne et la fonction de covariance :\n",
    "$$Z \\sim GP(m, k) \\:\\:\\:\\: \\mbox{avec} \\:\\:\\:\\: \\begin{cases}m(x) = 0  \\\\ k(x, x') = \\mbox{exp}(-\\frac{1}{2l}(x - x')^2), l > 0\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a482638-966a-4b10-be49-c7b7d99dd6d3",
   "metadata": {},
   "source": [
    "Ainsi, on peut définir une distribution *a priori* sur le modèle de régression, qui s'exprime, pour une collection de points $\\boldsymbol{x} = \\{x_1, \\ldots, x_N\\}$, comme suit :\n",
    "$$p(Z(\\boldsymbol{x}) \\vert \\mathcal{M}) = \\mathcal{N}(m(\\boldsymbol{x}), k(\\boldsymbol{x}, \\boldsymbol{x}))$$\n",
    "où \n",
    "$$m(\\boldsymbol{x}) = \\begin{bmatrix} m(x_1) \\\\ \\vdots \\\\ m(x_N)\\end{bmatrix} \\:\\:\\:\\: \\mbox{et} \\:\\:\\:\\: k(\\boldsymbol{x}, \\boldsymbol{x}) = \\begin{bmatrix} k(x_1, x_1) & k(x_1, x_2) & \\ldots & k(x_1, x_N) \\\\ k(x_2, x_1) & k(x_2, x_2) & \\ldots & k(x_2, x_N) \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ k(x_N, x_1) & k(x_N, x_2) & \\ldots & k(x_N, x_N)\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9131054-0dad-4e07-88ce-9b2f62a4736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    def __init__(self, mean, cov, noise = 0.1):\n",
    "        self.mean = mean\n",
    "        self.cov = cov\n",
    "        self.noise = noise\n",
    "\n",
    "    def prior(self, x):\n",
    "        mean = self.mean(x)\n",
    "        cov = self.cov(x)\n",
    "        return np.random.multivariate_normal(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9cd30-ed47-4a9b-9dd1-07b3a30d64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mean(x):\n",
    "    return np.zeros_like(x)\n",
    "\n",
    "def rbf_kernel(x, scale=0.1):\n",
    "    return np.exp(- 0.5 * (x[:, np.newaxis] - x[np.newaxis, :])**2 / scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610d183-74f6-45fc-bed6-a3344f6b06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 2))\n",
    "\n",
    "gp = GaussianProcess(zero_mean, rbf_kernel)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    gp_sample = gp.prior(x_test)\n",
    "    ax[i].plot(x_test, gp_sample, color=colors[i])\n",
    "    ax[i].set_xlabel('x')\n",
    "    if i == 0:\n",
    "        ax[i].set_ylabel('Z(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d700c4f-6003-4878-9579-98cfeae676bd",
   "metadata": {},
   "source": [
    "De manière analogue à la régression Bayésienne, on peut définir la distribution *a posteriori* sur le modèle étant donné les données.\n",
    "\n",
    "$$p(Z(\\boldsymbol{x}^\\ast) \\vert \\boldsymbol{x}, \\boldsymbol{y}, \\mathcal{M}) = \\mathcal{N}(m_{post}(\\boldsymbol{x}^\\ast), k_{post}(\\boldsymbol{x}^\\ast, \\boldsymbol{x}^\\ast))$$\n",
    "\n",
    "où $m_{post}$ et $k_{post}$ sont obtenus par application du théorème de conditionnement Gaussien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b63f1-5643-4dca-9e70-a7bc916c30ea",
   "metadata": {},
   "source": [
    "**Théorème de conditionnement Gaussien**\n",
    "\n",
    "Soit un vecteur Gaussien de taille $(n_1 + n_2) \\times 1$ \n",
    "$$\\begin{bmatrix} Y_1 \\\\ Y_2 \\end{bmatrix}$$\n",
    "où $Y_1$ est de taille $n_1 \\times 1$ et $Y_2$ est de taille $n_2 \\times 1$. On note son vecteur moyen\n",
    "$$\\begin{bmatrix} m_1 \\\\ m_2 \\end{bmatrix}$$\n",
    "et sa matrice de covariance\n",
    "$$\\begin{bmatrix} \\Sigma_1 & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{2} \\end{bmatrix}$$\n",
    "où $\\Sigma_1$ est de taille $(n_1 \\times n_1)$, $\\Sigma_{12}$ est de taille $(n_1 \\times n_2)$, $\\Sigma_{21}$ est de taille $(n_2 \\times n_1)$, et $\\Sigma_2$ est de taille $(n_2 \\times n_2)$. \n",
    "\n",
    "On suppose que $\\Sigma_1$ est inversible. Alors, pour tout $y \\in \\mathbb{R}^{n_1}$, le vecteur aléatoire $Y_2$, conditionnellement à $Y_1 = y_1$, est un vecteur Gaussien avec pour moyenne\n",
    "$$\\mathbb{E}[Y_2 \\vert Y_1 = y_1] = m_2 + \\Sigma_{12}^T\\Sigma_1^{-1}(y_1 - m_1)$$\n",
    "et pour matrice de covariance\n",
    "$$\\mbox{Cov}[Y_2 \\vert Y_1 = y_1] = \\Sigma_2 - \\Sigma_{12}^T\\Sigma_1^{-1}\\Sigma_{12}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563b9c1-89a4-497e-a6f0-95edf050f1ea",
   "metadata": {},
   "source": [
    "*Calculer la moyenne de $Y_2$ conditionnellement à $Y_1 = y_1$ si $y_1$ est égal à la moyenne de $Y_1$. Calculer la moyenne et la covariance de $Y_2$ conditionnellement à $Y_1 = y_1$ si $Y_1$ et $Y_2$ sont indépendants. De manière générale, que peut-on dire sur la covariance de $Y_2$ conditionnellement à $Y_1 = y_1$ par rapport à la covariance inconditionnelle de $Y_2$ ?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a820f-5a01-4248-92bd-0685be5997db",
   "metadata": {},
   "source": [
    "Ainsi, la distribution prédictive pour une nouvelle donnée $x^\\ast$ est :\n",
    "$$p(y^\\ast \\vert x^\\ast, \\boldsymbol{x}, \\boldsymbol{y}, \\mathcal{M}) = \\mathcal{N}\\big(y^\\ast; k(x^\\ast, \\boldsymbol{x})^T(k(\\boldsymbol{x}, \\boldsymbol{x}) + \\sigma^2 \\boldsymbol{I})^{-1}\\boldsymbol{y}, k(x^\\ast, x^\\ast) + \\sigma^2 -  k(x^\\ast, \\boldsymbol{x})^T(k(\\boldsymbol{x}, \\boldsymbol{x}) + \\sigma^2 \\boldsymbol{I})^{-1}k(x^\\ast, \\boldsymbol{x})\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c481724-d18e-483e-a8e7-f20d122b4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    def __init__(self, mean, cov, noise=1e-3):\n",
    "        self.mean = mean\n",
    "        self.cov = cov\n",
    "        self.noise = noise\n",
    "\n",
    "    def prior(self, x):\n",
    "        mean = self.mean(x)\n",
    "        cov = self.cov(x)\n",
    "        return np.random.multivariate_normal(mean, cov)\n",
    "\n",
    "    def posterior(self, x1, y1, x2, p_int=0.95):\n",
    "        n1 = x1.shape[0]\n",
    "        cov = self.cov(np.concatenate([x1, x2], axis=0))\n",
    "        cov_1 = cov[:n1, :n1]\n",
    "        cov_12 = cov[:n1, n1:]\n",
    "        cov_2 = cov[n1:, n1:]\n",
    "        prod = np.matmul(cov_12.T, np.linalg.inv(cov_1 + self.noise * np.eye(cov_1.shape[0])))\n",
    "        posterior_m2 = self.mean(x2) + prod.dot(y1 - self.mean(x1))\n",
    "        posterior_cov2 = cov_2 + self.noise - prod.dot(cov_12)\n",
    "        sample = np.random.multivariate_normal(posterior_m2, posterior_cov2)\n",
    "        Q = norm.isf((1-p_int) / 2)\n",
    "        l = Q * np.sqrt(np.diag(posterior_cov2) + self.noise)\n",
    "        low = posterior_m2 - l\n",
    "        high = posterior_m2 + l\n",
    "        return posterior_m2, low, high, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cf88f-fe67-43d3-ac14-0c4e6f828e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcess(zero_mean, rbf_kernel)\n",
    "\n",
    "n_samples = 4\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 2))\n",
    "\n",
    "x_test = np.linspace(x_train.min() - 0.2, x_train.max() + 0.2, 100)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    ax[i].scatter(x_train, y_train)\n",
    "    mean, low, high, sample = gp.posterior(x_train, y_train, x_test)\n",
    "    ax[i].plot(x_test, sample, color=colors[i])\n",
    "    ax[i].set_xlabel('x')\n",
    "    if i == 0:\n",
    "        ax[i].set_ylabel('Z(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c49a2-8505-4e06-879b-3ddef26f2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(x_train.min() - 1, x_train.max() + 1, 100)\n",
    "mean, low, high, sample = gp.posterior(x_train, y_train, x_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(x_train, y_train)\n",
    "mean = ax.plot(x_test, mean, color='black', label='Espérance')\n",
    "ax.fill_between(x_test, low, high, alpha=0.2, color='orange')\n",
    "patch = mpatches.Patch(color='orange', alpha=0.2, label=r'Intervalle de prédiction à {}%'.format(p_int*100))\n",
    "plt.legend(handles=[mean[0], patch])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(r'$\\hat{y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc67c75-2736-412b-9b95-44d9678c2065",
   "metadata": {},
   "source": [
    "**Vecteur Gaussien**\n",
    "\n",
    "Soit $V$ un vecteur aléatoire gaussien dans $\\mathbb{R}^n$. Les deux assertions sont équivalentes : \n",
    "* Toute combinaison linéaire de $V$ suit une distribution gaussienne. Autrement dit, pour tout vecteur $a$ fixé de dimension $n \\times $1, il existe $\\mu \\in \\mathbb{R}$ et $\\sigma^2 \\geq 0$ tels que $$a^TV = \\sum_{i=1}^N a_i V_i \\sim \\mathcal{N}(\\mu, \\sigma^2),$$\n",
    "* Il existe un vecteur $m$ de dimension $n \\times 1$, une matrice $K$ de dimension $n \\times r$, avec $r \\leq n$, et un\n",
    "vecteur $w$ de dimension $r \\times 1$ avec des composantes indépendantes suivant une distribution $\\mathcal{N}(0,1)$,\n",
    "tels que $V = m + Kw$. De plus,\n",
    "$$\\mathbb{E}[V] = m \\:\\:\\:\\: \\mbox{et} \\:\\:\\:\\: \\mbox{Cov}(V) = KK^T.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4602b9-8314-46d5-8327-e83d460da9a5",
   "metadata": {},
   "source": [
    "**Exercice de cours 1** \n",
    "\n",
    "Soient $X_1$ et $X_2$ deux variables aléatoires indépendantes avec une distribution $\\mathcal{N}(0, 1)$. Soit\n",
    "$$Z(x) = X_1 + \\mbox{cos}(x)X_2$$\n",
    "pour $x \\in [0, 1]$. Montrer que $Z$ est un processus Gaussien sur $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb141ee-1f1c-405e-bda2-3d9b39dfae6c",
   "metadata": {},
   "source": [
    "**Exercice de cours 2**\n",
    "\n",
    "On considère trois variables aléatoires indépendantes et identiquement distribuées $A, B, C$ de loi $\\mathcal{N}(0, 1)$. \n",
    "On pose, pour tout $(t, s) \\in [0, 1]^2,$\n",
    "$$Y(t, s) = tsA + (s − 2t)B + C.$$\n",
    "* Montrer que $Y$ est un processus Gaussien sur $[0, 1]^2$.\n",
    "* Quelle est la fonction moyenne de $Y$ ?\n",
    "* Quelle est la fonction de covariance de $Y$ ?\n",
    "* Quelle est la loi de $Y(1,1)$ ?\n",
    "* Quelle est la loi du vecteur $(Y(1,1), Y(0,1))$ conditionnellement à $Y(1, 0) = 2$ ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
