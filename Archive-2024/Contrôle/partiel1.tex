\documentclass{article}
\pagestyle{empty}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[french]{babel}
\usepackage{stmaryrd}
%\usepackage[latin1]{inputenc}
\setlength{\oddsidemargin}{-0.54cm}
\topmargin -2.54cm
\textwidth 17cm
\textheight 27cm
\newcommand{\BBr}{\mathbb{R}}
\newcommand{\BBn}{\mathbb{N}}
\newcommand{\BBp}{\mathbb{P}}
\newcommand{\BBe}{\mathbb{E}}
\newcommand{\Xn}{X_{n}}
\newcommand{\XXn}{X_{1},\ldots,\Xn}
\newcommand{\usn}{\frac{1}{n}}
\newcommand{\ust}{\frac{1}{3}}
\newcommand{\prs}[2] {\langle{#1},{#2}\rangle}
\newcommand{\Xtr}{X_{\text{train}}}
\newcommand{\Xts}{X_{\text{test}}}
\newcommand{\ytr}{y_{\text{train}}}
\newcommand{\yts}{y_{\text{test}}}
\newcommand{\lb}{\llbracket}
\newcommand{\rb}{\rrbracket}
%
\DeclareMathOperator{\logit}{logit}
%
\newcommand{\fonction}[5]{
 #1: \begin{array}{rcl}
	  #2 & \longrightarrow & #3 \\
     #4 & \longmapsto & #5 \end{array}
    }
%
\begin{document}
\noindent {\sc L FLEX}
\hfill 2023--2024\\
\noindent Introduction au Machine Learning 
\vspace{1cm}

\begin{center}
{\bf\large \'Epreuve du 2 avril 2024} \\
{\it Durée 1h30. Les notes de cours et les calculatrices sont autorisées}
\end{center}
%

\section*{Autour des méthodes de classification}
Soit $\mathcal{C}$ le carré $[-1,1]\times [-1,1]$ du plan. On considère le trois points du plan suivants~: $A=\begin{pmatrix}
-1\\0
\end{pmatrix}$, $B=\begin{pmatrix}
1\\0
\end{pmatrix}$ et $C=\begin{pmatrix}
0\\1
\end{pmatrix}$. Soit pour $x\in\mathcal{C}$, $Y(x)\in\{-1,1\}$ un label que l'on souhaite prédire. On sait que $Y(A)=Y(B)=-1$ et $Y(C)=1$.
\begin{enumerate}
\item Dessiner dans $\mathcal{C}$ les cellules de Voronoï associées aux trois points $A,B,C$. On expliquera avec clarté comment cette partition de $\mathcal{C}$ a été obtenue. 
\item On appelle $D$ le point intérieur au triangle $ABC$ qui est à égale distance des points $A$, $B$ et $C$. On rappelle que $D$ est le point d'intersection des médiatrices aux segments $[A,B]$, $[B,C]$ et $[C,A]$. Déterminer le points $D$. Soit $\widehat{Y}$ le prédicteur au $1$ plus proche voisin qui utilise l'échantillon d'apprentissage $A,B,C$. Soit $x=\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}\in\mathcal{C}$, montrer que $\widehat{Y}(x)=-1$ si 
$x_2<\max(x_1, -x_1)$ et $\widehat{Y}(x)=1$ sinon. 
%$x_2<\max(x_1, -x_1)$. 
Soit
$\widehat{\widehat{Y}}(x)$ le prédicteur au $3$ plus proches voisins de $x$. Que vaut $\widehat{\widehat{Y}}(x)$? Représenter graphiquement sur $\mathcal{C}$, avec des couleurs et sur deux schémas différents, les prédicteurs $\widehat{Y}$ et $\widehat{\widehat{Y}}$.
\item En préambule, on rappelle que l'aire d'un triangle est égale au demi produit de sa base par sa hauteur.
Soit $\theta\in]0,1[$ fixé. On suppose que $\BBp(Y=-1)=1-\BBp(Y=1)=\theta$.
On suppose, de plus, que la loi de $X$ conditionnelle à $\{Y=-1\}$ est uniforme sur $$\mathcal{C}_1=\left\{x=\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}\in\mathcal{C}: x_2>\max(x_1, -x_1)\right \},$$
et que la loi de $X$ conditionnelle à $\{Y=1\}$ est uniforme sur $\mathcal{C}$.
Représenter graphiquement, sur deux schémas différents, la densité de $X$ conditionnelle à $Y$.
%$$\mathcal{C}_2=\left\{x=\begin{pmatrix}
%x_1\\
%x_2
%\end{pmatrix}\in\mathcal{C}: x_2<\max(x_1, -x_1)\right \}.$$
Montrer que les probabilités d'erreur de prédiction $e(\widehat{Y},\theta)$ 
et $e(\widehat{\widehat{Y}},\theta)$  des prédicteurs $\widehat{Y}$ et
$\widehat{\widehat{Y}}$ sont $e(\widehat{Y},\theta)=\frac{\theta}{4}$ et
$e(\widehat{\widehat{Y}},\theta)=1-\theta$.
\item Pour $\theta\in]0,1[$, tracer les fonctions $e(\widehat{Y},\theta)$ et $e(\widehat{\widehat{Y}},\theta)$ et commenter les courbes obtenues.  
\item On s'intéresse  maintenant au classifieur bâti sur $A,B,C$ à l'aide de la méthode SVM. Pour $x\in\mathcal{C}$, on note $\widehat{\widehat{\widehat{Y}}}(x)$ le prédicteur ainsi obtenu. Montrer ou admettre que la frontière de prédiction SVM est ici une droite parallèle au segment $[A,B]$ et déterminer  cette frontière. Expliciter alors
$\widehat{\widehat{\widehat{Y}}}(x)$ et tracer la fonction  $e(\widehat{\widehat{\widehat{Y}}},\theta)$  
\end{enumerate}
%
\end{document}
